#filter to counties in southern Idaho
#filter(asd_desc %in% regions) %>%
filter(class_desc %in% age_var)  %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi, county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, value_char =value_trim, unit_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
census_api_key("6fd2754dd1bdcc811b51c669667df2873b3bd56e")
library(tidycensus)
install.packages("tidycensus")
library(httr)
library(jsonlite)
library(tidycensus)
library(tidyverse)
library(purrr)
library(mapview)
library(dplyr)
# If you've never used your tidycensus API key in your R session, run this:
census_api_key("6fd2754dd1bdcc811b51c669667df2873b3bd56e")
nass_key <- "B5240598-2A7D-38EE-BF8D-816A27BEF504" #QuickStats
# NASS url
nass_url <- "http://quickstats.nass.usda.gov"
# commodity description of interest
my_commodity_desc<- "FARM OPERATIONS" #[AG LAND, INCL BUILDINGS - OPERATIONS WITH ASSET VALUE, MEASURED IN $ / ACRE; $ / OPERATION; $/ACRE; $]; [AG LAND, CROPLAND, PASTURED ONLY - ACRES] [Income, Net or Farm-related?]
# query start year
my_year <- "2000"
# state of interest
my_state <- "ID"
###--------------------------------------#
# Download data and turn into dataframe
#####
# final path string
path_id_farms <- paste0("api/api_GET/?key=", nass_key, "&commodity_desc=", my_commodity_desc, "&year__GE=", my_year, "&state_alpha=", my_state)
#unpack JSON object
raw_id_farms <- GET(url = nass_url, path = path_id_farms)
char_raw_id_farms<- rawToChar(raw_id_farms$content)
# check size of object
nchar(char_raw_id_farms)
#turn into list
list_raw_id_farms<- fromJSON(char_raw_id_farms)
# apply rbind to each row of the list and convert to a data frame
id_farms_raw_data <- pmap_dfr(list_raw_id_farms, rbind)
#####--------------------------------------#
# Subset Data to State Level Aggreegates
#####
id_state_agg <- id_farms_raw_data %>%
filter(agg_level_desc == "STATE") %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi,
agg_level_desc, year, class_desc, domain_desc, domaincat_desc, value_char =value_trim, unit_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char)
#####--------------------------------------#
# Subset Data to County Level Aggreegates
#####
regions <- c("EAST", "SOUTHWEST", "SOUTH CENTRAL")
id_county_agg <- id_farms_raw_data %>%
filter(agg_level_desc == "COUNTY") %>%
filter(asd_desc %in% regions) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi,county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, domain_desc, domaincat_desc, value_char =value_trim, unit_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
install.packages("stringr", type="source")
library("stringr", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
View(list_raw_id_farms)
utils:::menuInstallPkgs(type="source")
regions <- c("EAST", "SOUTHWEST", "SOUTH CENTRAL")
id_county_agg <- id_farms_raw_data %>%
filter(agg_level_desc == "COUNTY") %>%
filter(asd_desc %in% regions) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
#mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi,county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, domain_desc, domaincat_desc, Value, unit_desc) %>% #value_char =value_trim
# filter out entries with codes '(D)' and '(Z)'
#filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
#mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
#select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
regions <- c("EAST", "SOUTHWEST", "SOUTH CENTRAL")
id_county_agg <- id_farms_raw_data %>%
filter(agg_level_desc == "COUNTY") %>%
filter(asd_desc %in% regions) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
#mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi,county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, domain_desc, domaincat_desc, Value, unit_desc) %>% #value_char =value_trim
# filter out entries with codes '(D)' and '(Z)'
#filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
#mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
#select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
#mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
View(id_county_agg)
unique(id_county_agg$domain_desc)
unique(id_county_agg$domaincat_desc)
unique(id_county_agg$class_desc)
regions <- c("EAST", "SOUTHWEST", "SOUTH CENTRAL")
id_county_agg <- id_farms_raw_data %>%
filter(agg_level_desc == "STATE") %>%
filter(asd_desc %in% regions) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
#mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi,county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, domain_desc, domaincat_desc, Value, unit_desc) %>% #value_char =value_trim
# filter out entries with codes '(D)' and '(Z)'
#filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
#mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
#select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
#mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
View(id_county_agg)
View(id_county_agg)
# Download and subset NASS AG Census Data
# Created following https://sheilasaia.rbind.io/post/2019-01-04-nass-api/
library(httr)
library(jsonlite)
library(tidycensus)
library(tidyverse)
library(purrr)
library(mapview)
library(dplyr)
# If you've never used your tidycensus API key in your R session, run this:
census_api_key("6fd2754dd1bdcc811b51c669667df2873b3bd56e")
nass_key <- "B5240598-2A7D-38EE-BF8D-816A27BEF504" #QuickStats
# NASS url
nass_url <- "http://quickstats.nass.usda.gov"
# commodity description of interest
my_commodity_desc<- "OPERATORS" #FARM OPERATIONS, [AG LAND, INCL BUILDINGS - OPERATIONS WITH ASSET VALUE, MEASURED IN $ / ACRE; $ / OPERATION; $/ACRE; $]; [AG LAND, CROPLAND, PASTURED ONLY - ACRES] [Income, Net or Farm-related?]
# query start year
my_year <- "2000"
# state of interest
my_state <- "ID"
###--------------------------------------#
# Download data and turn into dataframe
#####
# final path string
path_id_ops <- paste0("api/api_GET/?key=", nass_key, "&commodity_desc=", my_commodity_desc, "&year__GE=", my_year, "&state_alpha=", my_state)
#unpack JSON object
raw_id_ops <- GET(url = nass_url, path = path_id_ops)
char_raw_id_ops<- rawToChar(raw_id_ops$content)
# check size of object
nchar(char_raw_id_ops)
#turn into list
list_raw_id_ops<- fromJSON(char_raw_id_ops)
# apply rbind to each row of the list and convert to a data frame
id_ops_raw_data <- pmap_dfr(list_raw_id_ops, rbind)
###--------------------------------------#
# Subset Data to number of operators in each county, and age groups at state level
#####
regions <- c("EAST", "SOUTHWEST", "SOUTH CENTRAL")
All_cat<- unique(id_ops_raw_data$class_desc) #age categories
variables<-c("(ALL)", "(ALL), FEMALE", All_cat[3:9])
id_operators <- id_ops_raw_data %>%
#filter to specific data
#filter(class_desc %in% variables)  %>%
#filter(asd_desc %in% regions) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi, county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, domain_desc, domaincat_desc, value_char =value_trim, unit_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
ages<- id_operators %>%
filter( year == 2007) %>%
filter(class_desc %in% All_cat[3:9])%>%
select(-state_ansi, -county_name, -county_code, -asd_desc, -county_year, -GEOID)
All_cat[3:9]
install.packages("data.table")
install.packages("maxnet")
install.packages("xgboost")
library( dplyr )
rm(list=ls())
##### Create a social network of agents
# generate initial population
x <- 1:50
y <- 1:50
# number of iterations
numstep <- 50
numcell <- 2500
pop <- expand.grid(x, y)
View(pop)
names(pop) <- c("x", "y")
library(pse)
install.packages("Hmisc")
install.packages("pse")
library(pse)
install.packages(pse)
install.packages("pse")
install.packages("pse")
library(pse)
install.packages("pse")
install.packages("pse")
library(pse)
#define probability function
qdunif<-function(p, min, max){
floor(qunif(p, min, max))}
#wrap model
modelRun<-function(params){
return(mapply(outflowStor, params[,1], params[,2]))
}
#set parameters
q.arg<- list(list("min"=1, "max"=10), list("min"=1, "max"=15))
names(q.arg)<-c("s", "m")
factors<-c("s", "m")
#create hypercube
bothLHS <-LHS(model = NULL, factors, N=100, q='qdunif', q.arg, nboot=4)
bothparams<-bothLHS$data
#set only S to change
q.arg<- list(list("min"=1, "max"=10), list("min"=6, "max"=8))
sLHS<-LHS(model = NULL, factors, N=50, q='qdunif', q.arg, nboot=4)
Sparams<-sLHS$data
#set only M to change
q.arg<- list(list("min"=2, "max"=6), list("min"=1, "max"=15))
mLHS<-LHS(model = NULL, factors, N=100, q='qdunif', q.arg, nboot=4)
Mparams<-mLHS$data
out<-modelRun(bothparams)
out_S<-modelRun(Sparams)
out_M<-modelRun(Mparams)
#import, sum storage for Anderson Ranch, Arrowrock, and Lucky Peak Reservoirs
setwd("~/Documents/GitRepos/ReservoirModeling")
#setwd("D:/Documents/GitRepos/ReservoirModeling")
#--------------------------------------
# Import reservoir data
#--------------------------------------
# missing data was replaced by average of bracketing values
res<-read.csv("Data/BRB_reservoir_data_1997-2018_noleap.csv")
res$Date <-as.Date(res$Date, format ="%m/%d/%y")
res$totAF<- res$andAF +res$arkAF +res$lucAF
res$qoV<-res$qo*24*60*60*.0000229569 #convert from flow rate (cfs) to volume (acre-feet)
res$resid<- res$in_unreg- res$in_computed
res$Y = as.numeric(format(res$Date, format = "%Y"))
res$M = as.numeric(format(res$Date, format = "%m"))
res$WY[res$M <10]= res$Y[res$M <10]
res$WY[res$M >= 10] = res$Y[res$M >= 10]+1
lowell<-read.csv("Data/lowell_data.csv")
res$low<-lowell$low_af
# load Rule Curve data --------
fv<-read.csv("Data/ForecastVol.csv")
fcVol<-read.csv("Data/FloodControlVol.csv", header=FALSE) #DOY, date, volumes
fcVol[,3:36]<- fcVol[,3:36]*1000 #bc flood control space is in 1000 ac-ft see plate 7-1 in the WCM
fcVol$V2<- as.Date(fcVol$V2, format ="%m/%d/%Y")
prj<-read.csv("Data/Inflw_prj.csv") #coefficients for projection eqn
prjAP<-read.csv("Data/Inflw_prjAPril1.csv") #coefficients for projection eqn after April 1
wfc<-read.csv("Data/plate7-2.csv") #winter flood control space
qlim<- read.csv("Data/MaxQ.csv", header = FALSE) #discharge mins and maxes
#conversions from vol to flow (ac-ft to cfs)
v2f<-43560.000443512/(24*60*60)
f2v<-24*60*60*.0000229569
#Reservoir Storage from WCM
minQ<-240
maxAF<-1010188
minS<-  41000 +11630+ 28767 #total inactive capacity AND, ARK, LP
#subset (DOY 1: July 31st) from full timeseries -----
jul=196 #change to 212 #july 31st once update fcVol csv
reps <-100
idx<-which(res$doy >= 1 & res$doy <= jul)
rows<- length(idx)
FC<- data.frame(res$doy[idx],res$WY[idx], res$totAF[idx], res$in_unreg[idx], res$qo[idx])
colnames(FC)<-c("doy", "WY", "AF", "Q", "Qo")
yrs<- 1998:2018
#calculate daily forecast values
forecast<-read.csv("Data/LP_coordinatedForecasts.csv")
volF<-vector(length = nrow(FC))
for (i in 1:nrow(FC)){
if (any(forecast$wy == FC$WY[i] & forecast$doy == FC$doy[i])){
ii=which(forecast$wy == FC$WY[i] & forecast$doy == FC$doy[i])
volF[i] <- forecast$ForecastVol[ii]
} else{volF[i] <- volF[i-1] - FC$Q[i-1]*f2v}
}
FC<-cbind(FC, volF)
FC$volF[FC$volF < 0] <- 0
#find the index of the first doy
doy1<-matrix(data=NA, ncol=1, nrow=21)
for (wy in 1:21){
doy1[wy]<- which(FC$WY == yrs[wy] & FC$doy == 1)
}
##--------------------------------------
#       DEFINE FUNCTIONS
##--------------------------------------
##INITALIZE storF with the intial storage of the reservoir
# REQUIRED storage - lookup day of year, inflow volume ----
# plate 7-1 and plate 7-3 - Needs sum of timeseries
reqStor<- function(sumQin,doy){
fvol<-round(sumQin/1000000, digits=1)
fcol<- as.numeric(fv$col[fv$fv == fvol])
fcs<- as.numeric(fcVol[doy, fcol+2])
#Winter flood control space (low flow years Plate 7-2) ----
if (doy < 91 && fvol > 1.2 && fvol < 1.8){
wcol<- as.numeric(fv$wcol[fv$wfc == fvol])
fcs<- as.numeric(wfc[doy,wcol])
}
if (is.na(fcs) == TRUE){
fcs<- as.numeric(fcVol[doy, fcol+2])
}
#required flood control space (storage volume)
return(fcs)
}
#predict max storage in the next m days
predMaxS<- function(m){
for (day in 1:jul){
volF <- FC$volF[FC$WY == yrs[wy] & FC$doy == day] #todays forecasted inflow
count=0
if (volF >= 0 && day < (jul-m)){
for (it in day:(day+m-1)){
count=count+1
vF = volF - (volF/(jul-day))*(day-it) #predict maxS given equal distribution of inflow
reqS <- reqStor(vF, it)
maxS[day, count] <- (maxAF-reqS) #max storage today given the whole years inflow
}
} else {storD <- 0}
maxS[is.na(maxS)]<- maxAF
}
return(maxS)
}
#determine minimum daily release before April 1
minRelease<- function(day, volF){
ix= which(prj$start <= day & prj$end >= day)
vol1 = volF*prj$b[ix] + prj$c[ix]
if (prj$start[ix] != day){
vol2 = volF*prj$b[ix+1] + prj$c[ix+1]
frac = (day - prj$start[ix])/(prj$end[ix]-prj$start[ix])
volFmar <-  frac*vol1 + (1-frac)*vol2
} else {volFmar = vol1}
volFresid <- volF - volFmar
FCvolAP<- reqStor(volFresid, 91) #flood control space required on april 1
minEvac<- FCvolAP - availStor[day] #minimum evaculation btw today and April 1
minReleaseVol <- minEvac+volFmar
Qmin <- (minReleaseVol*v2f)/(jul-day+1) #associated  qmin
##If statements that constrain for high flows and ramp rates?
}
#determine minimum daily release after April 1
minReleaseApril<-function(day, volF){
ix30 = findInterval(day, prjAP$doy)
ix15=ix30-1
volF_target15 <- volF*prjAP$b[ix15] + prjAP$c[ix15]
volF_target30 <- volF*prjAP$b[ix30] + prjAP$c[ix30]
residual15<- volF-volF_target15
residual30<- volF- volF_target30
if (day < jul-30){
FCvol30<- reqStor(residual30, day+30) #required storage space
minEvac30 <- FCvol30 - availStor[day]
minReleaseVol30 <- minEvac30 + volF_target30
q30<-(minReleaseVol30*v2f)/(jul-day+1)
} else {q30 <- minQ}
if (day < jul-15){
FCvol15<-reqStor(residual15, day+15)
minEvac15 <- FCvol15 - availStor[day]
minReleaseVol15 <- minEvac15 + volF_target15
q15<-(minReleaseVol15*v2f)/(jul-day+1)
} else {q15 <- minQ}
Qmin <- max(q15, q30)
}
#forecast what the storage would be in s days given previous âˆ† in S and make those changes over m days
forecastS<-function(s,m,day){
if (day > s+1){
dsdt= (stor[day] - stor[day-s])/s
storF[day] <<- (dsdt*m)+stor[day]
} else {storF[day]<<- stor[day]}
}
#evaluate change in storage in regard to the forecasted storage to prevent going over maxS
#update discharge, change in storage and day+1 storage
evalS<- function(Qin, day, stor, maxS, Qmin,s,m){
if (storF[day] >= maxS[day,m] && day > s+1){ #&& day <188
dsdtMax= (storF[day] - maxS[day,m])/s
qo[day] <- Qmin[day] + (dsdtMax*v2f)
flag = 'TRUE' #true we need to increase ramp rates to get rid of the water
} else {qo[day] <- Qmin[day]
flag='FALSE'}
if (stor[day] > maxAF){
addQ = (stor[day] - maxAF)*v2f
minFCq[day] <- minFCq[day] + addQ
flag= 'TRUE'
} else {flag='FALSE'}
#if the calculated discharge is greater than +/- 500 set it to +/- 500
if (flag == 'TRUE'){ #going over maxS or maxAF
ramp= 1000
} else {ramp = 500}
if (qo[day] > (qo[day-1] + ramp) && day > 2 ){
qo[day] <- qo[day-1] + ramp
} else if (qo[day] < (qo[day-1]-500) && day > 2 ){
qo[day] <- qo[day-1]-500}
###UPDATE here
#this puts a hard constraints on not topping the dam - but doesnt work 21 times
if (availStor[day] <= (1000*v2f)){
qo[day] <- qo[day] + 1000
availStor[day] <- availStor[day] + 1000*f2v
stor[day] <- stor[day] - (1000*f2v)
}
dS[day] <- (Qin[day]- qo[day])*f2v
#dont let storage go below the minimum
if (stor[day] <= minS){
qo[day] <- minQ
dS[day] <- -minQ*f2v
}
#update storage for the next day
if (day < jul){
stor[day+1]<<-stor[day] + dS[day]  #AF in the reservoir
}
#write out values
qo[day]<<-qo[day]
dS[day]<<-dS[day]
}
#determine change in storage and outflow for a given water year and forecast window
outflowStor<-function(s,m){
results<-list()
discharge<-matrix(data=NA, nrow = jul, ncol=21)
for (wy in 1:21){
#   set up blank matricies
#------------------------------------------
stor<<-matrix(data=NA, nrow = jul, ncol = 1)
maxS<<-matrix(data=NA, nrow = jul, ncol = m)
availStor<<-matrix(data=NA, nrow = jul, ncol = 1)
minFCq<<-matrix(data=NA, nrow = jul, ncol = 1)
storF<<-matrix(data=NA, nrow = jul, ncol = 1)
qo<<-matrix(data=NA, nrow = jul, ncol = 1) #modeled outflow from reservoir
dS<<-matrix(data=NA, nrow = jul, ncol = 1)
#---------
# initalize
#-----
stor[1] <<- FC$AF[doy1[wy]] #initialize with actual storage on Jan 1
Qin<- FC$Q[FC$WY == yrs[wy]]
maxS <<- predMaxS(m) #vector of 198 days of max storage out to M days
#----- run all the functions to get to discharge and updated storage
for (day in 1:jul){
volF<<- FC$volF[FC$WY == yrs[wy] & FC$doy == day] #todays forecasted inflow
availStor[day] <<- maxAF-stor[day]
# Min flood control release for storage goals on April 1 and every 15 days after
if (day < 91){
minFCq[day]<<-minRelease(day, volF)
} else {
minFCq[day]<<-minReleaseApril(day, volF)
}
#minimum discharge is 240
if (minFCq[day] < minQ){
minFCq[day] <<- minQ
}
forecastS(s,m,day)
evalS(Qin, day, stor, maxS, minFCq, s, m)
}
#out<<- cbind(maxS[,1],availStor, storF, stor, dS, minFCq, qo)
#colnames(out)<-c('maxS', 'availStor', 'storF', 'stor', 'dS', 'minFCq', 'qo')
#results[[wy]]<-out
discharge[,wy]<-qo
}
Q<-c(discharge)
return(Q)
}
#set only S to change
q.arg<- list(list("min"=1, "max"=10), list("min"=6, "max"=8))
sLHS<-LHS(model = NULL, factors, N=50, q='qdunif', q.arg, nboot=4)
Sparams<-sLHS$data
#set only M to change
q.arg<- list(list("min"=2, "max"=6), list("min"=1, "max"=15))
mLHS<-LHS(model = NULL, factors, N=100, q='qdunif', q.arg, nboot=4)
Mparams<-mLHS$data
out<-modelRun(bothparams)
out_S<-modelRun(Sparams)
